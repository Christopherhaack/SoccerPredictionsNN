{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models.ipynb\n",
    "\n",
    "## This is the file where all of my models will go\n",
    "\n",
    "### The code below is used to convert loaded data into x and y vectors \n",
    "\n",
    "data is loaded from game objects into x and y vectors. The x vector will store info about the previous games for both the home and away team of the current game. It also will store information about the strength of opposition in each of these games and the player selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vLen = 734\n",
    "def timeStepX(x):\n",
    "    xRet = []\n",
    "    for xt in x:\n",
    "        xTemp = []\n",
    "        for i in range(5):\n",
    "            h = xt[i * vLen: (i +1) * vLen]\n",
    "            a = xt[(i+5) * vLen: (i + 6) * vLen]\n",
    "            val = h + a\n",
    "            xTemp.append(val)\n",
    "        xRet.append(xTemp)\n",
    "    return xRet\n",
    "def resVal(y):\n",
    "    yVals = []\n",
    "    for val in y:\n",
    "        if val[0] == 1:\n",
    "            yVals.append(3)\n",
    "        elif val[1] == 1:\n",
    "            yVals.append(1)\n",
    "        else:\n",
    "            yVals.append(0)\n",
    "    return yVals\n",
    "\n",
    "#changes the output of a game to a one hot encoding [win, tie, loss]\n",
    "def resOneHot(y):\n",
    "    if y == 3:\n",
    "        return [1 , 0, 0]\n",
    "    elif y == 1:\n",
    "        return [0, 1, 0]\n",
    "    else:\n",
    "        return [0, 0, 1]\n",
    "\n",
    "#creates a flat version of the x as a 1 x 8240 matrix and the y's as one hot vectors.\n",
    "def createDataSet2(gameInfo, season):\n",
    "    \n",
    "    ''' code used to create x and y values from a given season and set of games\n",
    "    creates a y value that is listed as a one hot vector\n",
    "     and x as a 1 x 8240 matrix that give information about the past 5 games'''\n",
    "    x = []\n",
    "    y = []\n",
    "    cnt = 0\n",
    "    teamDic = dict()\n",
    "    teams = season.teams\n",
    "    #this code block is meant to fix a mistake in the definition of my data structure, should really make performances\n",
    "    # a dictionary which each team name as a key, and the performances as a val. \n",
    "    for team in teams:\n",
    "        teamDic[team] = cnt\n",
    "        cnt += 1\n",
    "    for game in gameInfo:\n",
    "        example = []\n",
    "        stage = game[0]\n",
    "        res = game[5]\n",
    "        #the result of the game will constitute our y set. 0 = home loss, 1 = home draw, 3 = home win.\n",
    "        y.append(resOneHot(res))\n",
    "        home = game[1]\n",
    "        away = game[2]\n",
    "        homeLoc = teamDic[home]\n",
    "        awayLoc = teamDic[away]\n",
    "        #a training example will be a 10 * 824, with the home teams most recent 5 games as the first 5 values\n",
    "        #followed by the away teams most recent 5 games.\n",
    "        for i in range((stage-6), (stage - 1)):\n",
    "            game = season.performances[homeLoc].performance[i]\n",
    "            #first two items in game are the stage and away team api Id which I don't use in this\n",
    "            # model\n",
    "            game1 = game[2:]\n",
    "            if len(game1) < 734:\n",
    "                for k in range(len(game1), 734):\n",
    "                    game1.append(0)\n",
    "            if len(game1) != 734:        \n",
    "                print(len(game1))\n",
    "            for d in game1:\n",
    "                example.append(d)\n",
    "        for j in range((stage-6), (stage - 1)):\n",
    "            game = season.performances[awayLoc].performance[i]\n",
    "            game1 = game[2:]\n",
    "            if len(game1) < 734:\n",
    "                for k in range(len(game1), 734):\n",
    "                    game1.append(0)\n",
    "            if len(game1) != 734:        \n",
    "                print(len(game1))\n",
    "            for d in game1:\n",
    "                example.append(d)\n",
    "        x.append(example)\n",
    "    return x, y\n",
    "\n",
    "def generateSets(seasons):\n",
    "    trainX = []\n",
    "    trainY = []\n",
    "    testX = []\n",
    "    testY = []\n",
    "    for i in range(6):\n",
    "        season1 = seasons.seasonData[i].pVals[0]\n",
    "        numTeams = len(season1.teams)\n",
    "        n = int(numTeams / 2 *  5)\n",
    "        t = season1.info[n:]\n",
    "        x ,y = createDataSet2(t, season1)\n",
    "        trainX = trainX + x\n",
    "        trainY = trainY + y\n",
    "    for j in range(2):\n",
    "        season1 = seasons.seasonData[j + 6].pVals[0]\n",
    "        numTeams = len(season1.teams)\n",
    "        t = season1.info[n:]\n",
    "        x ,y = createDataSet2(t, season1)\n",
    "        testX = testX + x\n",
    "        testY = testY + y\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "def filterX(tx):\n",
    "    xRet = []\n",
    "    for x in tx:\n",
    "        xTemp = []\n",
    "        for i in x:\n",
    "            if type(i) != type(1) and type(i) != type(1.):\n",
    "                xTemp.append(0)\n",
    "            else:\n",
    "                xTemp.append(i)\n",
    "        xRet.append(xTemp)\n",
    "    return xRet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "this is where data is loaded from the pickle files with the information stored in them and converted to train and test data for each league"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7506 2502\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('soccerPerformanceEPL.pickle', 'rb') as handle:\n",
    "    prem = pickle.load(handle)\n",
    "with open('soccerPerformanceSL.pickle', 'rb') as handle:\n",
    "    ll = pickle.load(handle)\n",
    "with open('soccerPerformanceSWL.pickle', 'rb') as handle:\n",
    "    swl = pickle.load(handle)\n",
    "with open('soccerPerformanceFL.pickle', 'rb') as handle:\n",
    "    fl = pickle.load(handle)\n",
    "with open('soccerPerformanceNL.pickle', 'rb') as handle:\n",
    "    nl = pickle.load(handle)\n",
    "trainX1, trainY1, testX1, testY1 = generateSets(prem)\n",
    "trainX2, trainY2, testX2, testY2 = generateSets(ll)\n",
    "trainX3, trainY3, testX3, testY3 = generateSets(nl)\n",
    "trainX4, trainY4, testX4, testY4 = generateSets(fl)\n",
    "#trainX4, trainY4, testX4, testY4 = generateSets(swl)\n",
    "trainX = trainX1 + trainX2 + trainX3 + trainX4 \n",
    "trainY = trainY1 + trainY2 + trainY3 + trainY4\n",
    "testX = testX1 + testX2 + testX3 + testX4 \n",
    "testY = testY1 + testY2 + testY3 + testY4 \n",
    "trainX = filterX(trainX)\n",
    "testX = filterX(testX)\n",
    "\n",
    "print(len(trainY), len(testY))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "this is an attempt at running a convolutional neural network on the data. right now it is defunct, may go back and edit this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import random as rand\n",
    "POOL = 4\n",
    "W1 = 32\n",
    "W2 = 16\n",
    "W3 = 8\n",
    "WF1 = 256\n",
    "NUMFEATURES = 3\n",
    "D1 = 32 #depths\n",
    "D2 = 64\n",
    "D3 = 128\n",
    "\n",
    "\n",
    "def cw(nLayers, p, w= 824):\n",
    "    ''' calculates the width for fully connected layer\n",
    "    args are number of layers,\n",
    "    pool size, and the width of the argument default to 824'''\n",
    "    w1 = float(w)\n",
    "    for i in range(nLayers):\n",
    "        w1 = math.ceil(w1/p)\n",
    "    return w1\n",
    "W_FINAL = cw(3, POOL)\n",
    "\n",
    "def genBatch(x, y, n =100, shuffle = True):\n",
    "    x1 = []\n",
    "    y1 = []\n",
    "    a = []\n",
    "    for j in range(len(y)):\n",
    "        a.append(j)\n",
    "    if shuffle:\n",
    "        rand.shuffle(a)\n",
    "    for i in range(n):\n",
    "        j = a[i]\n",
    "        x1.append(x[j])\n",
    "        y1.append(y[j])\n",
    "    return x1, y1\n",
    "    \n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 4, 2, 1],\n",
    "                        strides=[1, 4, 2, 1], padding='SAME')\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 8240])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "\n",
    "W_conv1 = weight_variable([W1, 10, 1, D1])\n",
    "b_conv1 = bias_variable([D1])\n",
    "x_image = tf.reshape(x, [-1,824,10,1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([W2, 4, D1, D2])\n",
    "b_conv2 = bias_variable([D2])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_conv3 = weight_variable([W3, 2, D2, D3])\n",
    "b_conv3 = bias_variable([D3])\n",
    "\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "h_pool3 = max_pool_2x2(h_conv3)\n",
    "\n",
    "W_fc1 = weight_variable([W_FINAL * 2 * D3, WF1])\n",
    "b_fc1 = bias_variable([WF1])\n",
    "h_pool3_flat = tf.reshape(h_pool3, [-1,W_FINAL * 2 * D3])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([256, 3])\n",
    "b_fc2 = bias_variable([3])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(1000):\n",
    "    x_batch, y_batch = genBatch(trainX, trainY) \n",
    "  \n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x:x_batch, y_: y_batch, keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "        train_step.run(feed_dict={x: x_batch, y_:y_batch, keep_prob: 0.5})\n",
    "acc = 0\n",
    "for i in range(18):\n",
    "    x_btest = testX[i * 139: (i+1) * 139]\n",
    "    y_btest = testY[i * 139: (i+1) * 139]\n",
    "    temp = accuracy.eval(feed_dict={x: x_btest, y_: y_btest, keep_prob: 1.0})\n",
    "    acc += temp / 18.0\n",
    "print('accuracy is %g'%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test distribution \n",
    "\n",
    "this cell gives information about the test distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4572342126298961 0.24620303756994405 0.2965627498001599\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s = 0\n",
    "s1 = 0\n",
    "s2 = 0\n",
    "for i in range(len(testY)):\n",
    "    if testY[i][0] == 1:\n",
    "        s+= 1\n",
    "    elif testY[i][1] == 1:\n",
    "        s1 += 1\n",
    "    else:\n",
    "        s2 += 1\n",
    "print(s/len(testY), s1/len(testY), s2/len(testY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "this is a random forest classifier that predicts values based on our training set. It is currently our best model but we are hoping to beat its performance with an RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.521183053557\n"
     ]
    }
   ],
   "source": [
    "#first model that is better than a naive guess woot 46% is naive!!!\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "trainY2 = resVal(trainY)\n",
    "testY2 = resVal(testY)\n",
    "clf = RandomForestClassifier(n_estimators=150)\n",
    "clf.fit(trainX, trainY2)\n",
    "print(clf.score(trainX, trainY2))\n",
    "print(clf.score(testX, testY2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN MODEL\n",
    "\n",
    "this is the part of the code that trains a lstm rnn model on the simple data set above. It takes input as a time sequence\n",
    "which is a 5 * 12 vector denoting 12 stats for the previous 5 games, the home team result goals scored and goals against, and strength of the oppsotion, and similiarly the away teams result goals scored and goals against and strength of opposition. \n",
    "It then has to 512 * 512 nodes in which it learns features and dependencies and finaly this is brought together in a 3 softmax classificaiton. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Epoch 1/1\n",
      "7506/7506 [==============================] - 5s - loss: 1.2074     \n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Epoch 1/1\n",
      "7506/7506 [==============================] - 3s - loss: 1.0634     \n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Epoch 1/1\n",
      "7506/7506 [==============================] - 3s - loss: 1.0630     \n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Epoch 1/1\n",
      "7506/7506 [==============================] - 3s - loss: 1.0594     \n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Epoch 1/1\n",
      "7506/7506 [==============================] - 3s - loss: 1.0559     \n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "Epoch 1/1\n",
      "7506/7506 [==============================] - 3s - loss: 1.0567     \n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "Epoch 1/1\n",
      "7506/7506 [==============================] - 3s - loss: 1.0547     \n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "Epoch 1/1\n",
      "7506/7506 [==============================] - 3s - loss: 1.0546     \n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "Epoch 1/1\n",
      "7506/7506 [==============================] - 3s - loss: 1.0533     \n",
      "2502\n"
     ]
    }
   ],
   "source": [
    "if 'session' in locals() and session is not None:\n",
    "    print('Close interactive session')\n",
    "    session.close()\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(5, 1468)))\n",
    "model.add(Dropout(0))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "trainX1 = timeStepX(trainX)\n",
    "testX1 = timeStepX(testX)\n",
    "\n",
    "for iteration in range(1, 10):\n",
    "    print\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(trainX1, trainY, batch_size=139, nb_epoch=1)\n",
    "score = model.evaluate(testX1, testY, verbose=0)\n",
    "yVals = model.predict_on_batch(testX1)\n",
    "ytVals = model.predict(trainX1, batch_size=32)\n",
    "print(len(yVals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN TEST PERFORMANCE\n",
    "\n",
    "this maps the test performance of the recurrent neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4572342126298961\n"
     ]
    }
   ],
   "source": [
    "s4 = 0\n",
    "\n",
    "    \n",
    "\n",
    "for i in range(len(yVals)):\n",
    "    y = yVals[i]\n",
    "    y = list(y)\n",
    "    idx = y.index(max(y))\n",
    "    if testY[i][idx] == 1:\n",
    "        s4 += 1\n",
    "print(s4/2502)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this is code that I'm working on to create an RNN with just tensorflow \n",
    "\n",
    "still a work in progress, will come back to this shortly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "lstm_size = 20\n",
    "batch_size = 50\n",
    "sess = tf.InteractiveSession()\n",
    "lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "# Initial state of the LSTM memory.\n",
    "state = tf.zeros([batch_size, lstm_size])\n",
    "probabilities = []\n",
    "loss = 0.0\n",
    "\n",
    "trainX1 = timeStepX(trainX)\n",
    "testX1 = timeStepX(testX)\n",
    "\n",
    "for game in trainX1:\n",
    "    \n",
    "    # The value of state is updated after processing each batch of words.\n",
    "    output, state = lstm(game, state)\n",
    "\n",
    "    # The LSTM output can be used to make next word predictions\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    probabilities.append(tf.nn.softmax(logits))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
